{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LINEAR REGRESSION\n",
        "PyTorch Coding Exercises\n",
        "Given: 12 October 2022 Deadline: 15 Oct 2022\n",
        "Activity 1: Learn a Regression Model from Data\n",
        "With an understanding of the basics of PyTorch, you are ready to start building and training neural network \n",
        "models. In this activity, you will create a simple regression model that can predict a student's grade based on \n",
        "the number of hours they have studied and slept the day before a test is given.\n",
        "The network you implement should have two input units (the two input dimensions are hours of study time \n",
        "and hours of sleep, as integers between 0-9), three hidden units, and one output unit (the predicted grade on \n",
        "a continuous scale between 0-1). Use a sigmoid function as the non-linear activation function in the hidden \n",
        "units.\n",
        "The table below contains the training data you should use (scale these values as necessary). Note the trend \n",
        "that is present: more sleep and more study time leads to a higher grade (it's true!). This is the pattern that \n",
        "should be learned by the network if it is working correctly.\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "Train your network for 1,000 epochs using a mean sum squared loss function. Your training code should \n",
        "incrementally print the loss value as training proceeds. What is the value of the loss in the final epoch? \n",
        "Now it's time to test the network. Produce predictions of test performance based on the following sequence of \n",
        "study and sleep hours. Your prediction code should print each test instance out to the terminal. Record each \n",
        "prediction in an CSV file.\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n"
      ],
      "metadata": {
        "id": "3p7OBZBDZ2vo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "PVRzRpO-QTFb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils import data\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn.functional as F\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_val=torch.from_numpy(np.array([[5,9],[4,8],[3,6],[5,8],[1,4],[2,6],[8,8],[7,8],[2,5],[6,9]]).astype(np.float32))\n",
        "output_val = torch.from_numpy(np.array([92,91,82,95,74,75,96,94,80,91]).astype(np.float32))\n"
      ],
      "metadata": {
        "id": "ONOEZlLTS2GY"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = TensorDataset(input_val, output_val)\n",
        "train_ds.tensors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yon0BtFjyqN5",
        "outputId": "37903404-053a-409a-9fd4-beda2ee7d4e1"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[5., 9.],\n",
              "         [4., 8.],\n",
              "         [3., 6.],\n",
              "         [5., 8.],\n",
              "         [1., 4.],\n",
              "         [2., 6.],\n",
              "         [8., 8.],\n",
              "         [7., 8.],\n",
              "         [2., 5.],\n",
              "         [6., 9.]]),\n",
              " tensor([92., 91., 82., 95., 74., 75., 96., 94., 80., 91.]))"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 5\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
        "next(iter(train_dl))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxc-owJyzDFo",
        "outputId": "8516635c-98a1-4cce-b2f1-b4c7192f1258"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[7., 8.],\n",
              "         [1., 4.],\n",
              "         [5., 8.],\n",
              "         [3., 6.],\n",
              "         [8., 8.]]), tensor([94., 74., 95., 82., 96.])]"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNet(nn.Module):\n",
        "    # Initialize the layers\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(2, 8)\n",
        "        self.act1 = nn.ReLU() # Activation function\n",
        "        self.linear2 = nn.Linear(8, 1)\n",
        "        #self.act2 = nn.ReLU() # Activation function\n",
        "        #self.linear3 = nn.Linear(3, 1)\n",
        "    \n",
        "    # Perform the computation\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.linear2(x)\n",
        "        #x = self.act2(x)\n",
        "        #x = self.linear3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "TXtqHBstYk5Y"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_l = SimpleNet()\n",
        "optimizer_m = torch.optim.Adam(model_l.parameters(), 1e-1)\n",
        "#loss_fn = nn.MSELoss()\n",
        "loss_fn = F.mse_loss\n"
      ],
      "metadata": {
        "id": "orFCZMw7Uxz6"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(num_epochs, model, loss_fn, opt):\n",
        "    for epoch in range(num_epochs):\n",
        "        for xb,yb in train_dl:\n",
        "            # Generate predictions\n",
        "            pred = model(xb)\n",
        "            loss = loss_fn(pred, yb)\n",
        "            # Perform gradient descent\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "        print('Training loss: ', loss_fn(pred, yb))"
      ],
      "metadata": {
        "id": "em-j-PzNMUPw"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model for 100 epochs\n",
        "fit(1500, model_l, loss_fn, optimizer_m)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjWPkJ7UMcTL",
        "outputId": "ecb17192-1b30-4481-b872-8d70b7e7b6a5"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: UserWarning: Using a target size (torch.Size([5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss:  tensor(7183.5107, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(6474.9570, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(5159.3882, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(3470.5198, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(2424.6946, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(1391.2024, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(251.9666, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(748.1552, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(800.9447, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(1367.3992, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(1264.2390, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(501.9005, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(651.4993, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(592.8486, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(752.2596, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(938.8061, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(794.1256, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(742.2534, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(566.4346, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(464.8238, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(390.9195, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(614.2704, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(369.9524, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(604.8044, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(337.2701, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(382.3482, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(431.1389, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(416.9785, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(492.3279, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(403.4703, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(404.0629, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(481.5560, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(231.7356, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(434.9904, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(157.8747, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(192.6573, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(258.8397, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(255.8719, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(264.3390, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(244.8723, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(344.4341, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(304.2522, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(314.7534, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(254.5257, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(222.0840, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(279.0835, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(178.3315, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(281.4658, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(116.2381, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(325.1835, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(174.6977, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(188.0232, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(168.5781, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(264.4840, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(179.5333, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(273.8351, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(152.9637, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(247.3446, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(119.7034, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(231.9601, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(131.7081, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(232.0416, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(225.4600, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(184.8212, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(197.1794, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(140.8222, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(141.3931, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(182.2462, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(173.6432, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(238.3288, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(151.2377, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(175.3263, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(124.5270, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(254.7510, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(152.5099, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(197.3919, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(154.4086, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(112.5021, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(173.3148, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(219.9009, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(94.0444, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(117.5252, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(157.4365, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(112.5179, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(109.6730, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(134.1525, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(167.1868, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(222.7608, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(97.5125, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(184.4830, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(212.3415, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(115.3239, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(194.7774, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(67.9868, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(198.3005, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(191.9137, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(177.2485, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(204.8873, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(119.3683, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(147.3734, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(173.3656, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(133.1754, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(118.5890, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(168.4023, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(134.8058, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(128.1343, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(113.4929, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(119.6131, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(161.1804, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(134.6870, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(97.7858, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(106.9981, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(170.5416, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(125.9905, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(106.4581, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(112.7569, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(152.1942, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(132.3103, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(97.2751, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(170.6648, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(138.3378, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(108.9460, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(138.3936, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(106.7618, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(100.2535, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(127.1093, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(96.0721, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(117.8826, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(102.3986, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(129.5391, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(157.7204, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(131.4855, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(118.3067, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(93.7827, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(92.4948, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(107.2891, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(109.3185, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(76.8139, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(77.8801, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(96.2899, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(111.0117, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(117.8137, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(140.8887, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(90.0948, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(107.5573, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(131.5556, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(129.7202, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(100.2664, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(115.8653, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(110.3574, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(150.0059, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(100.9514, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(104.3449, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(153.2476, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(90.7900, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(108.7976, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(133.6091, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(130.0885, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(99.5630, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(122.0863, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(88.3178, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(111.7687, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(89.0647, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(66.5748, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(91.8607, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(102.2595, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(97.0211, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(63.9093, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(134.3712, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(83.9783, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(96.9576, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(84.8138, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(93.4393, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(82.3017, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(51.0849, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(68.6901, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(81.5372, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(136.5914, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(96.6723, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(83.5791, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(73.5967, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(115.6648, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(133.1410, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(37.1485, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(84.5774, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(99.9750, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(77.3362, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(77.1216, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(31.2830, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(85.4565, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(78.0961, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(77.5074, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(58.1166, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(105.3474, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(117.6920, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(59.3037, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(67.3515, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(82.7897, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(100.3662, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(79.4691, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(73.2157, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(38.4983, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(100.2121, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(87.5223, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(77.3798, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(62.9475, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(72.8838, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(86.6617, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(72.1890, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(101.7882, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(74.3477, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(91.2913, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(57.3728, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(68.0495, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(79.8398, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(77.7078, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(88.0806, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(66.4100, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(55.3177, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(62.4770, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(33.5716, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(50.1057, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(99.0523, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(93.4421, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(50.3816, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(80.4017, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(98.8321, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(90.4394, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(28.6436, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(77.9141, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(75.6247, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(84.9734, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(62.1675, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(22.5097, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(82.6199, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(64.8759, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(71.0952, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(91.9934, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(94.0072, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(104.1941, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(53.6636, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(57.4381, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(49.3421, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(69.5621, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(44.8490, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(68.3073, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(36.8439, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(56.0908, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(65.8652, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(64.7906, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(70.7980, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(92.8232, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(63.5010, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(52.4530, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(66.3844, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(72.0529, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(85.4265, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(38.2712, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(77.8709, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(91.9187, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(41.5889, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(86.8739, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(60.3227, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(59.9070, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(76.4797, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(62.0111, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(62.4037, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(56.3675, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(44.4699, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(32.7048, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(35.8129, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(98.7639, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(48.7327, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(62.1737, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(74.9763, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(59.2989, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(22.2875, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(98.8720, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(40.8015, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(74.4066, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(72.0871, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(101.3392, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(125.0167, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(52.9791, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(66.0666, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(82.9712, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(106.4411, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(94.9429, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(60.9191, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(99.1439, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(84.9690, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(105.5302, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(58.7264, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(65.4780, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(80.4183, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(108.5287, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(62.7321, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(82.3427, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(79.0703, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(84.0458, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(71.7925, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(65.6449, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(73.0701, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(66.1605, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(83.4674, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(65.2377, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(65.8650, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(69.4134, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(42.0875, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(77.1794, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(103.9795, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(47.4423, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(48.3005, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(99.0062, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(46.7144, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(121.8360, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(78.3334, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(66.5459, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(54.3450, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(60.0990, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(52.9478, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(47.5923, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(64.2694, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(50.9579, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(65.8667, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(103.8916, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(95.0637, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(74.5076, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(82.6698, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(51.4762, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(79.2160, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(71.5418, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(69.2699, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(65.1630, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(57.7916, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(71.3601, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(62.4878, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(76.6375, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(76.7028, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(54.1411, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(51.2713, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(58.8396, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(64.2156, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(103.7901, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(60.7883, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(64.4353, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(79.8157, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(61.8384, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(83.9935, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(47.4805, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(75.4241, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(73.5894, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(85.5593, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(50.3133, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(70.0593, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(75.1579, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(85.1094, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(72.8472, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(64.1546, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(55.8939, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(60.0283, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(100.6725, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(84.4614, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(72.9156, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(85.9435, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(74.1733, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(60.9102, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(114.4614, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(16.4298, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(83.9807, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(47.6427, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(59.8168, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(100.9598, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(98.4452, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(71.1152, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(92.3030, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(87.9039, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(78.0695, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(59.5386, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(97.0589, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(66.3986, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(51.2225, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(82.3085, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(69.9775, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(56.2813, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(67.8421, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(47.1573, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(104.9235, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(60.9699, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(70.8444, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(71.3445, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(74.4960, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(63.0565, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(66.5159, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(63.3255, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(80.4568, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(68.8710, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(47.1986, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(71.1586, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(100.5023, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(43.9442, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(79.7282, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(50.6685, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(94.8651, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(75.9586, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(76.1204, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(96.2446, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(93.9497, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(74.9907, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(59.0087, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(57.8352, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(58.1580, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(53.7304, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(42.3018, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(67.9511, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(55.6153, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(80.6749, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(63.6608, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(81.6623, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(71.3434, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(57.9501, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(66.4458, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(66.5621, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(62.9680, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(51.8123, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(62.2050, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(58.6517, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(68.6764, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(47.6343, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(99.0199, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(100.3904, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(60.5503, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(49.5810, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(66.1229, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(56.1555, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(47.9330, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(51.5652, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(34.2239, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(95.2458, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(65.8563, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(81.2793, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(63.0599, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(99.0675, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(76.7510, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(67.6294, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(103.3206, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(75.7857, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(49.2911, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(65.7618, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(64.3922, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(98.4842, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(39.6987, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(74.1946, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(66.7089, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(60.2425, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(63.8158, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(39.2081, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(34.5707, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(47.8697, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(42.6152, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(64.7578, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(72.8354, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(55.1542, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(102.9751, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(115.2686, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(65.6522, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(65.9936, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(69.4168, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(69.6677, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(89.3669, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(70.2585, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(58.5134, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(98.2476, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(82.9819, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(58.7589, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(82.6273, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(78.2572, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(45.3482, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(43.4389, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(104.5233, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(111.6495, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(65.2586, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(68.7306, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(46.3264, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(109.7243, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(101.8265, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(60.6159, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(61.8498, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(59.0445, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(31.7263, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(129.3849, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(74.8123, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(70.8157, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(95.2156, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(32.3248, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(90.0677, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(82.7926, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(75.1207, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(133.0799, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(53.2639, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(120.7790, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(77.4337, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(41.9890, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(57.9972, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(40.0965, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(57.3615, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(56.6413, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(97.4099, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(78.8913, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(67.4990, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(100.0045, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(85.6261, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(91.1362, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(62.4160, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(78.2666, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(73.6729, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(73.5855, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(77.2675, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(84.2756, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(66.6613, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(91.9034, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(64.1564, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(53.6942, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(52.9639, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(77.8528, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(72.9887, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(83.6054, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(65.7298, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(65.0627, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(76.7070, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(63.6124, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(37.0319, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(115.5136, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(68.0590, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(50.3717, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(80.3467, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(96.2553, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(75.1972, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(67.1242, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(61.0198, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(111.2533, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(59.7343, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(27.2446, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(114.0674, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(65.3387, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(105.5088, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(59.4118, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(86.7115, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(112.2567, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(51.6055, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(82.5631, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(60.1765, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(29.0926, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(85.9907, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(69.6033, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(80.7506, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(87.9790, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(96.1499, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(40.6022, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(97.1596, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(77.2186, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(61.8450, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(104.4415, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(133.9191, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(68.8739, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(112.6637, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(59.8174, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(65.5699, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(32.0650, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(82.7678, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(65.2107, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(59.5687, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(71.2608, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(72.9439, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(52.3905, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(90.1696, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(79.8793, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(96.2385, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(70.0315, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(92.5445, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(75.9322, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(75.2628, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(59.3564, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(30.3827, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(66.0620, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(56.9023, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(42.3372, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(48.7397, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(60.4436, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(30.9206, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(70.7238, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(70.1532, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(88.5085, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(51.4045, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(71.3481, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(44.9037, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(62.2661, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(44.8517, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(63.4461, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(90.0809, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(84.0314, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(85.1490, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(81.9080, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(91.8893, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(67.4129, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(79.2352, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(101.1600, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(34.4810, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(86.1272, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(82.2136, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(91.1276, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(73.9279, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(61.9537, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(72.0415, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(83.8193, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(81.1097, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(79.2190, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(105.6946, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(36.8993, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(89.0003, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(65.1462, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(54.2103, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(41.0813, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(96.7029, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(59.6584, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(78.0266, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(100.3248, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(64.8341, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(91.0175, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(64.1763, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(76.9707, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(62.6919, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(49.4437, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(33.4292, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(50.3940, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(47.0774, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(62.4573, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(48.8432, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(40.0584, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(55.8489, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(76.8308, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(66.1172, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(100.6318, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(33.9044, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(72.7504, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(72.0168, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(65.7958, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(32.0444, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(97.8027, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(89.7015, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(59.1588, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(51.9035, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(90.0438, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(60.2373, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(60.4639, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(57.2790, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(69.5031, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(71.8496, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(36.0090, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(72.8739, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(94.2607, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(61.2842, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(57.0478, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(81.2506, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(54.3839, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(79.1776, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(83.8234, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(73.7277, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(66.5681, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(36.3515, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(84.0888, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(77.9733, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(61.3731, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(74.9381, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(60.3880, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(55.2785, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(87.4062, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(57.8532, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(27.1564, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(93.9590, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(71.3508, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(46.1452, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(72.2625, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(104.2691, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(58.5795, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(65.8546, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(61.2425, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(52.3845, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(103.4670, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(70.3003, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(85.0421, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(76.1244, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(64.7293, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(64.6258, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(65.9170, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(63.0661, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(30.0447, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(93.4481, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(89.1778, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(104.0971, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(54.4360, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(61.2946, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(55.2890, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(72.8673, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(74.2780, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(55.6326, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(48.3922, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(68.8933, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(76.5018, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(59.8298, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(94.7934, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(84.4799, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(91.7326, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(59.4870, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(71.3516, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(53.0855, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(40.5209, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(88.6491, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(33.3492, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(38.1681, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(91.2225, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(90.9250, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(95.9048, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(91.8203, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(37.6409, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(79.1687, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(88.5381, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(39.6138, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(59.4759, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(76.6339, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(75.1024, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(96.0378, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(64.4700, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(68.2489, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(63.2771, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(95.1566, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(104.6256, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(53.6634, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(83.0686, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(76.5837, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(117.2708, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(100.8020, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(86.1008, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(60.4149, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(39.7754, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(76.8260, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(38.5094, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(66.9612, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(60.0651, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(62.0883, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(28.3722, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(49.4990, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(87.3545, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(35.8077, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(90.8433, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(91.4849, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(39.5283, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(70.8593, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(76.8089, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(59.1473, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(120.0714, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(64.7599, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(88.5285, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(62.8866, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(29.3176, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(60.7833, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(64.1275, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(90.0092, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(58.7892, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(62.7847, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(35.3892, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(43.7929, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(70.8436, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(44.9339, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(61.5074, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(74.1106, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(63.6278, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(104.3788, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(63.8120, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(58.1350, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(90.1516, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(89.4941, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(54.4359, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(46.7376, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(63.6154, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(95.2617, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(63.6488, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(43.8406, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(68.4103, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(38.0234, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(78.6115, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(35.8444, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(62.6471, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(81.5533, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(43.4989, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(77.3267, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(22.6286, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(87.3127, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(68.9800, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(68.2778, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(69.3144, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(53.9553, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(68.6978, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(54.2010, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(58.6818, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(28.3317, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(32.9087, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(48.9877, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(72.3228, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(62.9938, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(43.5220, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(44.1583, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(63.9654, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(56.7011, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(59.3102, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(36.7225, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(36.2069, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(54.8781, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(78.5394, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(47.8073, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(60.2962, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(102.3405, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(86.3002, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(71.8771, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(73.2075, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(38.0174, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(62.5820, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(82.6661, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(76.3967, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(63.8036, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(67.9261, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(38.3779, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(66.5198, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(85.7009, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(83.0831, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(86.6494, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(46.0703, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(81.6666, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(41.2452, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(62.7772, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(77.7151, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(63.6236, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(68.7954, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(126.9131, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(60.4142, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(85.5608, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(77.6184, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(75.1178, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(65.4300, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(91.0520, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(90.6460, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(64.9028, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(49.7239, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(59.9613, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(67.6778, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(58.8135, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(36.7246, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(75.7933, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(67.3008, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(62.9034, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(97.3976, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(66.2207, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(49.4030, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(63.8485, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(33.1551, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(52.6915, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(71.3262, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(91.1825, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(63.3698, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(37.4965, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(41.7375, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(89.7265, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(64.7560, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(61.1304, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(94.7643, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(83.6391, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(56.6313, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(114.8532, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(39.3440, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(40.6583, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(83.0865, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(100.3593, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(68.7358, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(77.4304, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(39.8471, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(58.4455, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(67.9848, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(79.4726, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(79.6181, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(80.3428, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(80.3549, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(92.0415, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(61.3185, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(57.7690, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(101.3995, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(65.2485, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(72.7352, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(89.0651, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(41.0482, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(53.6677, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(61.8636, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(67.9386, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(53.4264, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(73.6454, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(52.4488, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(53.8830, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(58.3519, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(90.3074, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(76.1192, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(57.6207, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(42.3757, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(67.0047, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(95.5482, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(102.6895, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(73.0825, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(86.5219, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(92.3060, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(87.0209, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(67.1517, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(74.2168, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(96.1874, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(86.6565, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(70.5129, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(62.5799, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(51.5931, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(98.1698, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(71.5219, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(30.8094, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(65.0851, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(70.4616, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(62.0462, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(98.3817, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(52.2587, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(69.8582, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(81.0446, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(46.8578, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(95.8131, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(78.4591, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(90.4652, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(58.0971, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(55.0188, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(59.8591, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(46.3733, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(45.8291, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(99.8949, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(34.2828, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(65.2827, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(51.7722, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(48.5562, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(70.3470, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(105.1102, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(64.1427, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(70.4739, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(61.3202, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(73.9351, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(80.4258, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(88.4851, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(62.1523, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(109.0805, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(56.1220, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(70.6794, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(55.8194, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(74.8214, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(38.5577, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(30.6617, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(44.1699, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(71.3594, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(61.2600, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(98.1327, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(74.0779, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(87.8330, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(47.4897, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(89.4516, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(33.8971, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(97.8263, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(69.6393, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(97.5549, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(73.6066, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(66.1960, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(105.0385, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(89.3105, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(69.7221, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(102.8731, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(53.6076, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(65.9657, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(56.6397, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(66.7062, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(76.4744, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(96.0100, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(26.9010, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(46.3999, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(101.7332, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(63.0069, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(65.2752, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(92.1886, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(117.2365, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(60.0803, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(64.3751, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(50.6769, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(62.6829, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(85.0984, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(48.2036, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(55.5255, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(58.8461, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(62.0099, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(115.8138, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(126.5592, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(81.8512, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(68.8800, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(91.3299, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(65.5734, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(139.7376, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(42.9944, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(54.8091, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(71.8748, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(58.9392, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(111.4432, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(67.3778, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(78.2849, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(86.1657, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(66.7780, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(69.7082, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(99.7067, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(71.3482, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(79.9795, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(64.1247, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(67.6772, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(95.6642, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(97.8253, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(66.3967, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(103.8067, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(55.1311, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(23.5425, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(23.1925, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(78.0336, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(77.1565, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(46.6065, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(39.3945, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(36.9004, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(67.4377, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(63.8539, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(61.9068, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(65.4641, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(61.2844, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(72.8196, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(63.4222, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(44.9629, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(74.4575, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(69.4585, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(47.2403, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(81.4824, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(59.1477, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(99.2015, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(82.4182, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(85.2630, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(75.2229, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(68.8050, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(27.3619, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(60.3936, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(60.3626, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(72.9062, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(98.2846, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(112.4073, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(70.1787, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(67.2521, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(72.1990, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(105.7781, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(47.9265, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(79.7005, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(80.4641, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(56.3625, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(67.5191, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(64.0586, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(98.3254, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(72.6188, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(34.5316, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(91.5575, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(47.3726, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(75.6467, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(64.6489, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(64.5230, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(61.1181, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(62.0552, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(102.7124, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(37.6586, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(53.7755, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(71.2847, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(99.6646, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(106.5907, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(57.0072, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(60.8478, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(52.8151, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(111.0037, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(68.5476, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(58.9917, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(87.5901, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(56.8027, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(63.6007, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(40.3777, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(66.0395, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(103.5884, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(86.7684, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(63.7951, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(63.9560, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(98.0354, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(63.2290, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(66.1699, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(94.2849, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(45.7796, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(81.8942, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(56.3331, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(87.4743, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(66.2285, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(88.1147, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(94.6383, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(94.6478, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(46.1871, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(54.8883, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(61.6209, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(75.9657, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(56.3758, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(76.6593, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(71.1267, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(60.6346, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(104.0030, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(29.4585, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(70.8586, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(59.6894, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(69.6172, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(48.0675, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(76.8714, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(60.3504, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(90.6020, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(46.3730, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(93.7082, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(79.2055, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(33.2463, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(73.3689, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(70.2513, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(58.4334, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(43.1603, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(36.0395, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(97.1374, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(76.9889, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(66.8090, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(55.9171, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(71.1025, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(66.7579, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(99.8900, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(79.1327, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(60.2444, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(75.7108, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(62.7843, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(57.7756, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(53.4953, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(90.3161, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(65.1489, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(84.5375, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(80.6415, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(70.9579, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(52.7984, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(64.4991, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(62.5668, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(103.5109, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(87.1819, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(48.3003, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(71.0515, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(57.3473, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(75.4244, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(77.0257, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(69.9509, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(88.1325, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(74.9315, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(67.7565, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(86.7528, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(56.1193, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(74.7653, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(62.7621, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(57.9914, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(56.1558, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(63.9943, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(82.7359, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(93.3423, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(110.0177, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(56.8392, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(96.8863, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(38.3516, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(137.4297, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(55.5468, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(68.4131, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(66.7261, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(64.4567, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(35.4162, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(30.5207, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(73.3239, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(36.6819, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(58.8078, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(93.3438, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(56.0099, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(104.1124, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(85.4732, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(32.4619, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(98.1609, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(85.9093, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(73.9267, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(70.7750, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(29.0238, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(72.8685, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(73.4386, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(67.4114, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(55.4257, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(63.2323, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(76.3142, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(73.4538, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(64.7665, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(61.6308, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(90.0148, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(62.9863, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(80.3285, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(98.1840, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(57.6702, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(86.3425, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(60.4143, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(73.2205, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(84.8534, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(112.2647, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(67.6289, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(55.5879, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(104.5232, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(66.0694, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(131.8149, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(29.8755, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(74.1310, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(84.6623, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(44.7539, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(109.4470, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(94.6306, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(72.4918, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(80.6912, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(53.8815, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(68.6512, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(88.1385, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(71.6016, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(61.3141, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(77.6970, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(87.3832, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(74.8537, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(74.3520, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(78.6391, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(56.0167, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(117.4228, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(84.5252, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(56.1612, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(85.2168, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(85.4414, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(85.7718, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(72.3683, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(55.5688, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(91.5234, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(33.2345, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(63.7529, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(102.3982, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(80.9450, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(102.6496, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(84.0140, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(72.0126, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(114.5058, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(66.4908, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(102.9645, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(62.6856, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(105.5164, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(59.1380, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(55.3249, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(73.6505, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(47.7171, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(32.4758, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(71.9712, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(63.9920, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(47.5031, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(53.7626, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(78.7386, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(107.4820, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(70.2528, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(66.5520, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(72.8871, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(57.7730, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(67.0840, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(46.3885, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(86.9779, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(103.7029, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(71.0360, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(82.0128, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(64.9765, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(54.5679, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(28.0174, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(37.4189, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(51.6553, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(85.6088, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(85.2890, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(40.1558, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(70.1222, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(86.5558, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(50.7979, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(102.8187, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(69.5414, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(82.3417, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(69.8088, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(74.6568, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(78.1294, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(66.0656, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(76.5112, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(49.5062, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(39.4423, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(66.4203, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(61.2214, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(91.0793, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(80.6382, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(20.8237, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(113.2618, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(82.3611, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(70.8435, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(60.2412, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(78.5461, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(51.7470, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(63.7036, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(37.1658, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(70.5946, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(89.5956, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(53.6130, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(62.3430, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(110.7337, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(105.7976, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(78.6032, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(90.3338, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(59.7296, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(69.2852, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(69.0347, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(65.1513, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(103.2595, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(71.6674, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(70.3337, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(70.9068, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(52.5804, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(111.7108, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(81.2980, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(95.4164, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(67.0361, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(82.0266, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(86.6210, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(113.8300, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(119.9692, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(59.1172, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(61.9515, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(71.9378, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(57.6664, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(71.4727, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(49.7769, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(66.3866, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(64.3896, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(95.4539, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(34.8687, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(62.0590, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(76.0557, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(40.6497, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(52.2215, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(67.9598, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(45.2949, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(67.6218, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(70.6881, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(39.4874, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(61.2169, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(73.0670, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(71.5127, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(64.3937, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(71.6067, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(72.2001, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(53.7910, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(68.1884, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(76.3249, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(54.9618, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(61.3774, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(104.3471, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(84.2258, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(58.3451, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(65.9546, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(58.4434, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(61.2257, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(65.8239, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(64.2539, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(92.0757, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(56.1346, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(61.0913, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(71.1912, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(37.7759, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(47.9286, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(58.4422, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(69.4989, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(63.9159, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(60.2632, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(77.5589, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(67.1376, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(72.0752, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(96.2974, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(62.3308, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(63.4323, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(81.5713, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(47.3179, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(64.2113, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(62.5731, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(92.1101, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(75.9098, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(85.7489, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(77.9645, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(61.9991, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(67.8078, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(77.4112, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(59.7750, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(83.3378, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(76.4130, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(116.5020, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(119.0316, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(67.1605, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(106.6702, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(60.8631, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(36.2148, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(76.4576, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(78.6378, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(107.2811, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(51.8093, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(27.9201, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(101.8896, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(81.6115, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(49.2391, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(84.0224, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(100.7570, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(95.2419, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(90.2173, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(85.6829, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(103.1382, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(29.8530, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(70.1586, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(64.1621, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(75.7096, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(40.6346, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(27.9483, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(61.7968, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(59.4027, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(41.6140, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(60.5887, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(114.8288, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(55.1175, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(72.4612, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(94.9319, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(81.3514, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(61.9521, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(73.9602, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(77.7220, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(70.7200, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(74.5944, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(126.7999, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(67.3565, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(82.1915, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(79.7063, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(69.0775, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(90.4420, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(75.2536, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(71.7962, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(79.1710, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(88.9870, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(75.4981, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(55.2368, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(70.6150, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(90.3916, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(36.9031, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(67.5536, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(84.8053, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(68.5947, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(93.0553, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(50.3392, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(59.7713, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(95.0056, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(66.2384, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(84.1656, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(69.6141, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(60.3380, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(45.3615, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(70.8994, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(102.5367, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(77.3595, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(68.3703, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(81.4180, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(70.3628, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(55.2716, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(102.6280, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(75.4608, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(70.4965, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(102.5966, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(64.1227, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(64.8353, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(77.3915, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model using test sets\n",
        "with torch.no_grad():\n",
        "    X_test = torch.from_numpy(np.array([[3,9],[1,3],[7,7],[4,5],[8,9]]).astype(np.float32))\n",
        "    prediction = model_l(X_test)\n"
      ],
      "metadata": {
        "id": "ZY2GZJ5W2DES"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3r199WOEA0g",
        "outputId": "256d311d-b760-45e9-ae75-dffee53483ed"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[86.9055],\n",
              "        [84.8935],\n",
              "        [86.1738],\n",
              "        [85.5337],\n",
              "        [86.8400]])"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NAND\n",
        "Activity 2: Create a Network that can Learn the NAND Operator\n",
        "Recall from computer architecture that the NAND operator is a universal operator, meaning that any other \n",
        "logical operator can be expressed as a combination of NAND operations. Fundamentally, this operator itself is \n",
        "the combination of the operators NOT and AND. An interesting task from the perspective of AI is the ability to \n",
        "learn such an operator through example.\n",
        "The network you implement here should have two input units (the two Boolean values that are being \n",
        "processed by the NAND operator), two hidden units, and one output unit (the exact output of the NAND \n",
        "operator). Use a sigmoid function as the non-linear activation function in the hidden units. The output of your \n",
        "code should be the exact answer for any two input bits. Hint: since we are learning NAND from data, some \n",
        "post-processing may be needed on the prediction from the last layer of the network.\n",
        "Your training data for this activity is the truth table for the NAND operator:\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "train your network for 1,000 epochs using mean sum squared loss. Your training code should incrementally \n",
        "print the loss value as training proceeds. What is the value of the loss in the final epoch? Record the values.\n",
        "After training, you should be able to reproduce the above truth table exactly by providing A and B as the two \n",
        "inputs. Your program should print both inputs and the output for all possibilities. "
      ],
      "metadata": {
        "id": "tAInIbVDpJga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils import data\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "g3BO9rZPpHeb"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_val=torch.from_numpy(np.array([[0,0],[0,1],[1,0],[1,1]]).astype(np.float32))\n",
        "output_val = torch.from_numpy(np.array([1,1,1,0]).astype(np.float32))\n"
      ],
      "metadata": {
        "id": "n50sgS0BpEJD"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = TensorDataset(input_val, output_val)\n",
        "train_ds.tensors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2j64jkjpXn2",
        "outputId": "623785bb-933e-49b9-a788-f74d02d62171"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0., 0.],\n",
              "         [0., 1.],\n",
              "         [1., 0.],\n",
              "         [1., 1.]]), tensor([1., 1., 1., 0.]))"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
        "next(iter(train_dl))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wObG9Hg-pcWO",
        "outputId": "7a6c09b2-5735-49a3-f8f7-0f3cd7d0c95d"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[1., 0.]]), tensor([1.])]"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNet(nn.Module):\n",
        "    # Initialize the layers\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(2, 2)\n",
        "        self.act1 = nn.Sigmoid() # Activation function\n",
        "        self.linear2 = nn.Linear(2, 1)\n",
        "        self.act2 = nn.Sigmoid() # Activation function\n",
        "        #self.linear3 = nn.Linear(3, 1)\n",
        "    \n",
        "    # Perform the computation\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.act2(x)\n",
        "        #x = self.linear3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "zeRvGnlOpgHk"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "F.sigmoid(torch.tensor(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UJtoxGUroHO",
        "outputId": "3ebc5f01-b82d-4785-a6f8-97c0704a87e8"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.5000)"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_l = SimpleNet()\n",
        "optimizer_m = torch.optim.SGD(model_l.parameters(), 1e-1)\n",
        "#loss_fn = nn.MSELoss()\n",
        "loss_fn = F.mse_loss\n"
      ],
      "metadata": {
        "id": "hJaQU8FcpjMa"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fit(1000, model_l, loss_fn, optimizer_m)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYcn6hy0pm7t",
        "outputId": "4d55cb17-c74f-46fc-e18b-fd120d580576"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss:  tensor(0.3273, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.3417, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1706, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1610, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1600, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1602, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1536, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1474, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1274, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1295, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1182, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1139, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.4525, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.4585, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1042, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1012, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0985, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1020, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1075, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1052, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0897, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0880, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0995, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0846, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.5072, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0820, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0808, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0926, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0831, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0774, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.5217, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0801, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0877, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.5267, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0863, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0727, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.5310, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0752, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0835, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0703, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0697, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0727, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0721, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0715, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.5392, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0803, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0800, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0797, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0794, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0789, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0659, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0681, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0781, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0674, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0776, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0666, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0662, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0639, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0764, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0762, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0649, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0630, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.5465, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0642, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0756, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0754, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.5465, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.5462, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0752, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0628, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0751, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0619, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0749, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0616, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0618, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0614, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0746, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0611, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.5453, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.5448, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0609, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0606, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0609, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0608, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0601, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.5435, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0744, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0744, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.5424, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0745, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.5414, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.5408, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0593, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0746, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0746, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.5395, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0747, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0747, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0583, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0582, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0604, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.5374, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0604, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0748, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0748, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0747, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0602, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0572, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0602, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0569, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0601, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0565, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0564, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0563, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.5325, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0752, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0751, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0599, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0556, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0751, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.5299, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0753, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.5287, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0600, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0551, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0756, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0599, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0600, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0758, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0544, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.5248, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0600, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0540, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0539, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0599, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0760, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.5222, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0598, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0598, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0598, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0598, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0597, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0597, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.5189, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0766, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0599, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0599, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0598, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0768, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.5155, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.5146, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0517, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.5131, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.5123, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0603, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0779, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0512, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.5097, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0781, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0782, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0508, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0506, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0505, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.5063, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0503, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0787, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0605, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0499, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0790, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.5025, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0607, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0495, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0607, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0796, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0795, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.4989, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0797, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0607, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0798, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.4966, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0800, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0480, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.4947, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.4938, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0608, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0475, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.4922, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0473, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0472, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0609, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0609, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0466, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0811, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0608, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0461, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0459, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.4866, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0457, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.4851, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.4842, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.4834, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0453, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0610, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.4814, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0449, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0825, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0446, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0825, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0443, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0825, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0439, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0437, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0826, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0433, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.4751, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.4742, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.4732, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0611, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0611, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0833, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.4709, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0422, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0609, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.4692, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0418, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0610, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.4670, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0612, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0612, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0843, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0611, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0407, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0846, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0404, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0402, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0848, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.4601, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0612, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0611, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0611, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.4576, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0391, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.4562, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0854, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0386, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0610, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.4537, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0857, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0611, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0611, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0610, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.4503, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0374, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0372, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0863, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0864, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0865, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0866, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.4449, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0364, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0870, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0613, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.4418, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.4407, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0357, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.4391, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.4381, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0880, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0879, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0349, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0347, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0879, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0880, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0342, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0879, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.4327, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.4316, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.4306, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0886, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.4288, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0616, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0330, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0889, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0888, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.4255, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0323, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.4238, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0614, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0891, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.4219, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0892, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0314, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.4196, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0894, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.4181, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0896, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0306, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0896, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0612, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0612, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0898, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0611, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0898, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0610, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0897, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0290, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0289, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0287, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0898, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0899, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0608, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0281, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.4052, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0900, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.4035, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0276, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0903, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0273, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.4001, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0906, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0905, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0607, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0265, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.3965, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0262, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.3945, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0907, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0906, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0907, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0254, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0605, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0252, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0908, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0250, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0908, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.3865, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0245, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.3848, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0242, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0241, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0605, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.3814, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0237, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.3794, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0235, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0606, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0232, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0230, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0912, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0604, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0911, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.3730, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0604, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0603, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0910, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0600, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0908, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.3686, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0601, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0600, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.3658, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.3646, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.3634, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.3623, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0209, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.3601, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0206, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0914, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0604, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0602, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.3565, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0913, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0199, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.3535, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0602, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.3520, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0193, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0600, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0600, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.3487, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.3476, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0910, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0600, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0905, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0183, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.3443, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0901, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.3426, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0179, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0178, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0595, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0175, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0593, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0592, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0893, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.3361, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0591, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0168, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0890, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.3327, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0589, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.3309, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0162, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0160, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0586, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0158, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.3274, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0584, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.3256, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0584, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0873, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0581, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0150, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0149, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0866, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.3202, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0579, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0864, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.3172, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0863, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0862, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0859, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0577, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.3129, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0579, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0138, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0137, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0853, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0135, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0851, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.3066, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0577, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0848, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.3037, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0575, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0129, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0841, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0838, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0835, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0124, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0568, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0567, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2970, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0829, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0121, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0825, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0119, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2920, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0822, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0567, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0116, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0115, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0114, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2866, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0815, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2845, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0565, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0111, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0565, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0109, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0807, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0563, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0563, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0563, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0105, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2756, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2743, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0104, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0103, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0789, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0559, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0557, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2699, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0556, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0098, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0098, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0555, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2655, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2642, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0771, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0095, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0769, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2605, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0093, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2581, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0766, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0091, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2554, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0760, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0756, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0551, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0088, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2519, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0745, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0548, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2494, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0548, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2474, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2462, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2451, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0083, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0082, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0544, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0727, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2416, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0542, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0541, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0717, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2385, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0078, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0714, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0077, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0538, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0538, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0535, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0533, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0531, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0073, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0073, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2299, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0530, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0684, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0682, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0070, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0069, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0069, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0672, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2238, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0668, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2222, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0664, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0066, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0518, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2193, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0517, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0652, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2168, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0517, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0064, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0648, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0644, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0513, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2117, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2106, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2095, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2084, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0638, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0512, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0059, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0059, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2050, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0059, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0509, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0506, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0616, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0614, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0611, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0501, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0605, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0602, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0600, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0054, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0598, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0596, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0492, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0489, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1944, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0586, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0488, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0485, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0576, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1912, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0484, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0050, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0570, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1880, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0049, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0566, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0564, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0048, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0476, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1835, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0047, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0047, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0552, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0046, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0547, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0046, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0045, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0045, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1779, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0537, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0461, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0459, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0043, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1744, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0460, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1726, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1716, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0526, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0042, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0521, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0041, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0516, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0041, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1676, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1666, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0040, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0040, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0447, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0505, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0039, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0502, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1622, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0039, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0038, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1598, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0038, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1581, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0442, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0493, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0490, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0436, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0435, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1548, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1540, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0432, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0036, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1524, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0478, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0428, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0035, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0425, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1495, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0468, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0034, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1480, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1472, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0458, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1444, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0415, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0412, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0448, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0445, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0405, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1420, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0403, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0402, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0437, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0434, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1394, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0433, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0430, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1362, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0395, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1350, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1343, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0394, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0392, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0390, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1324, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1317, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0416, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0414, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1306, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0409, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0407, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0378, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0377, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0401, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0397, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1268, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0370, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0365, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0385, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0383, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0382, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0359, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0355, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1208, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0374, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0373, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1192, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0371, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0370, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1177, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1171, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0349, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0365, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0363, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1144, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0360, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0359, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1131, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0339, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0337, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0351, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1113, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1107, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0350, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0349, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1084, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1078, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1073, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0331, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1056, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1051, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0340, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0325, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0324, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0333, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0331, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1027, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0328, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0325, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0312, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0311, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0310, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0993, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0319, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0306, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0305, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0313, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0969, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0964, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0960, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0310, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0300, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0946, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0297, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0303, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0299, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0298, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0295, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0914, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0292, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0898, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0289, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0884, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0285, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0284, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0874, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0870, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0281, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0863, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0279, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0854, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0846, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0842, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0275, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0274, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0273, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0267, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0269, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0263, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0266, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0264, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0263, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0816, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0258, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0257, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0260, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0804, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0256, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0258, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0796, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0792, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0252, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0251, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0250, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0252, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0778, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0247, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0767, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0764, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0247, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0756, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0244, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0245, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0746, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0241, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0238, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0240, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0737, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0733, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0730, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0238, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0237, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0236, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0234, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0715, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0230, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0232, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0708, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0229, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0698, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0695, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0692, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0689, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0687, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0227, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0682, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0223, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0223, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0221, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0675, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0219, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0219, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0218, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0662, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0216, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0216, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0656, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0215, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0652, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0213, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0211, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0645, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0642, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0211, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0635, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0208, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0631, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0208, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0626, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0622, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0620, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0618, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0615, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0205, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0610, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0202, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0202, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0605, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0201, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0201, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0199, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0598, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0198, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0197, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0196, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0591, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0195, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0194, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0194, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0193, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0581, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0192, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0191, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0572, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0567, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0187, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0187, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0186, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0185, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0184, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0184, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0552, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0183, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0182, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0179, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0177, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0535, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0177, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0532, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0176, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0175, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0175, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0174, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0523, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0521, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0173, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0518, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0171, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0512, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0510, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0170, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0170, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0170, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0169, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0168, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0168, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0167, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0166, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0166, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0497, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0165, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0493, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0164, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0490, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0163, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0163, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0487, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0485, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0162, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0482, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0161, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0160, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0477, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0475, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0474, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0472, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0159, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0468, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0157, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0157, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0463, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0460, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0459, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0154, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0154, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.0007, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    X_test = torch.from_numpy(np.array([[0,0],[1,1],[1,0],[1,1],[0,1]]).astype(np.float32))\n",
        "    #Y_test = mnist_test.test_labels.to(device)\n",
        "\n",
        "    prediction = model_l(X_test)"
      ],
      "metadata": {
        "id": "YhZzyR3jprKR"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction.round().int()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9D8_HIy-pt5E",
        "outputId": "d4606256-97ee-4d3c-eaad-6e2d68e3a735"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1],\n",
              "        [0],\n",
              "        [1],\n",
              "        [0],\n",
              "        [1]], dtype=torch.int32)"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BANK FRAUD SECTION\n",
        "Activity 3: Build Three different Multi-Layer Perceptron Models for Bank Fraud Identification\n",
        "You are given a CSV file with Bank Fraud Identification dataset that contains 20k+ transactions with 112 \n",
        "features (numerical) and 1 Target Feature (targets).\n",
        "In this activity, you will create a simple regression model using three different variants of Multi-layer \n",
        "Perceptron Models can predict a Bank Fraud based on the various features available. Compare the \n",
        "performance of the models on the test data.\n",
        "First Spilt your dataset in the ratio of 80% for Training and remaining 20% for testing.\n",
        "Input of the model will take up 112 features and outputs a Binary value of either 0 or 1.\n",
        "You are free to use any loss function or optimizers to train your model.\n",
        "[link text](https://https://github.com/HKannan23/Pytorch_codes/blob/main/Datasets_file/fraud_detection_bank_dataset.csv)"
      ],
      "metadata": {
        "id": "cc0HRp27vJBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils import data\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "oZbvU43Sz2LA"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_read = pd.read_csv(\"/content/fraud_detection_bank_dataset.csv\")"
      ],
      "metadata": {
        "id": "WUrHnONXxi4A"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pd_slicing = csv_read.iloc[:,1:113]\n",
        "#np_arr = pd_slicing.to_numpy()\n",
        "#input_val=torch.from_numpy(np_arr).float()\n"
      ],
      "metadata": {
        "id": "CpAAAkt01hHx"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_read = csv_read.sample(frac=1, axis=0).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "czrF_hFSQ7rh"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pd_slicing.shape"
      ],
      "metadata": {
        "id": "ReauiCTSRAhD"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tr_len = round(len(pd_slicing)*80/100)"
      ],
      "metadata": {
        "id": "u7Xgr0BRQHk7"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_len = round(len(pd_slicing)*20/100)"
      ],
      "metadata": {
        "id": "Z0dvBiJGQP-d"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pd_slicing = csv_read.iloc[1:tr_len,1:113]\n",
        "#np_arr = pd_slicing.to_numpy()\n",
        "#input_val=torch.from_numpy(np_arr).float()"
      ],
      "metadata": {
        "id": "o7LovW8GQA6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pd_slicing.head()"
      ],
      "metadata": {
        "id": "UwqW1WMYRHsB"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pd_slicing2 = csv_read.iloc[:,-1]\n",
        "#np_arr2 = pd_slicing2.to_numpy()\n",
        "#output_val = torch.from_numpy(np_arr2).float()"
      ],
      "metadata": {
        "id": "qFrpI4_L44kn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pd_slicing2.head()"
      ],
      "metadata": {
        "id": "HHrbPcsZRgp1"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#csv_read.head()"
      ],
      "metadata": {
        "id": "KaGOVGJwRqdF"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#input_val.shape"
      ],
      "metadata": {
        "id": "9K81-xum5Uz3"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#output_val"
      ],
      "metadata": {
        "id": "mKV2jM9y5X6k"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tr_ds = TensorDataset(input_val, output_val)\n",
        "#tr_ds.tensors"
      ],
      "metadata": {
        "id": "vwWi95C75goR"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pd_slicing3 = csv_read.iloc[tr_len+1:tr_len+test_len,1:113]\n",
        "#np_arr = pd_slicing3.to_numpy()\n",
        "#input_val1=torch.from_numpy(np_arr).float()"
      ],
      "metadata": {
        "id": "Ml9d98U7PU6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pd_slicing4 = csv_read.iloc[tr_len+1:tr_len+test_len,-1]\n",
        "#np_arr = pd_slicing4.to_numpy()\n",
        "#output_val1=torch.from_numpy(np_arr).float()"
      ],
      "metadata": {
        "id": "MwtyIMyoSWj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test_ds = TensorDataset(input_val1, output_val1)\n",
        "#test_ds.tensors"
      ],
      "metadata": {
        "id": "xs5l2B14SniY"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd_slicing = csv_read.iloc[:,1:113]\n",
        "np_arr = pd_slicing.to_numpy()\n",
        "input_val2=torch.from_numpy(np_arr).float()\n",
        "\n",
        "pd_slicing = csv_read.iloc[:,-1]\n",
        "np_arr = pd_slicing.to_numpy()\n",
        "output_val2 = torch.from_numpy(np_arr).float()\n",
        "\n",
        "tensor_ds = TensorDataset(input_val2, output_val2)"
      ],
      "metadata": {
        "id": "VK0FUeMxY9Tn"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds, test_ds = torch.utils.data.random_split(tensor_ds, [16374, 4094])"
      ],
      "metadata": {
        "id": "9M19j0EkD1J3"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 100\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
        "next(iter(train_dl))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-TeqzfJ5n4d",
        "outputId": "42f34182-32e0-4f02-e125-7bf533b02687"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[8.0000e+00, 3.6500e+02, 9.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
              "          1.0000e+00],\n",
              "         [2.0000e+00, 2.4300e+02, 1.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
              "          6.2000e+01],\n",
              "         [0.0000e+00, 2.1000e+01, 0.0000e+00,  ..., 1.0000e+00, 0.0000e+00,\n",
              "          1.0500e+02],\n",
              "         ...,\n",
              "         [7.0000e+00, 2.0000e+02, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
              "          2.0000e+01],\n",
              "         [0.0000e+00, 2.5000e+01, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
              "          9.6000e+01],\n",
              "         [1.0000e+01, 2.1540e+03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
              "          6.6000e+01]]),\n",
              " tensor([0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1.,\n",
              "         0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
              "         0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
              "         1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
              "         1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1.,\n",
              "         0., 0., 0., 0., 0., 1., 0., 0., 1., 0.])]"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNet(nn.Module):\n",
        "    # Initialize the layers\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(112, 20)\n",
        "        self.act1 = nn.ReLU() # Activation function\n",
        "        self.linear2 = nn.Linear(20, 15)\n",
        "        self.act2 = nn.ReLU() # Activation function\n",
        "        self.linear3 = nn.Linear(15, 7)\n",
        "        self.act3 = nn.ReLU() # Activation function\n",
        "        self.linear4 = nn.Linear(7, 1)\n",
        "        self.act4 = nn.Sigmoid() # Activation function\n",
        "        \n",
        "        \n",
        "    \n",
        "    # Perform the computation\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.act2(x)\n",
        "        x = self.linear3(x)\n",
        "        x = self.act3(x)\n",
        "        x = self.linear4(x)\n",
        "        x = self.act4(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "jBXPhlRq5ylK"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_l = SimpleNet()\n",
        "optimizer_m = torch.optim.SGD(model_l.parameters(), 1e-5)\n",
        "#loss_fn = nn.MSELoss()\n",
        "loss_fn = F.mse_loss"
      ],
      "metadata": {
        "id": "uAAHclwc6AIb"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(num_epochs, model, loss_fn, opt):\n",
        "    for epoch in range(num_epochs):\n",
        "        for xb,yb in train_dl:\n",
        "            # Generate predictions\n",
        "            pred = model(xb)\n",
        "            loss = loss_fn(pred, yb)\n",
        "            # Perform gradient descent\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "        print('Training loss: ', loss_fn(pred, yb))"
      ],
      "metadata": {
        "id": "5MP4A17ROqkc"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fit(1000, model_l, loss_fn, optimizer_m)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7QMlHkt6BPz",
        "outputId": "ce8937c2-9f99-45c3-aa25-140cf082b2de"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: Using a target size (torch.Size([74])) that is different to the input size (torch.Size([74, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: UserWarning: Using a target size (torch.Size([74])) that is different to the input size (torch.Size([74, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss:  tensor(0.2459, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2319, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2511, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2223, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2055, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2545, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2303, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2347, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1884, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2630, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2039, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2446, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2343, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2346, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2192, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2193, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2462, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1728, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1968, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2215, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2293, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1805, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2431, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2102, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1537, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2107, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2278, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2370, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2665, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2371, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2437, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2661, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2180, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2476, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2050, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2472, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2014, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2369, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2519, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2120, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2329, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2029, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2235, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2706, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2415, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2224, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2570, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1831, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2336, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2025, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2535, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2090, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2489, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1641, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2025, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2205, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2254, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1881, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1981, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2939, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2203, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1897, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2194, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2348, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2239, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2451, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2085, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2042, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2507, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2067, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2269, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1877, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1746, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2137, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2779, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2600, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2396, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2123, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2289, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2082, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2381, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2314, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2061, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2325, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2322, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2165, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2759, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2134, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2663, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2187, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2237, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2545, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1991, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1883, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2040, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2772, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2342, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1898, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2059, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2293, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2552, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2434, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2549, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2221, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2055, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1674, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2270, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2144, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2325, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2216, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2214, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2006, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2350, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2498, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2590, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2162, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2004, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2174, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2394, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1744, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2280, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1715, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2539, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2137, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2496, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2137, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1957, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1827, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2585, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2482, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2118, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2401, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2372, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2120, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1967, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2278, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2088, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2472, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1739, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1841, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2007, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2528, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1995, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1871, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1978, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2208, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2176, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2348, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1879, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2244, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2760, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2578, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1993, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2148, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2102, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2632, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2579, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2465, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2044, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2452, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1961, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2214, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1981, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2735, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2563, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2216, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1900, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2061, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2551, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2316, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2460, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1856, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2191, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2227, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2393, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2116, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2365, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2028, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2063, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1717, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2802, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2405, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2225, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2090, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1887, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1965, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2881, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2321, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1703, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2233, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2203, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1947, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2546, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2032, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2223, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1825, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2398, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2183, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2294, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2369, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2480, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2180, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2123, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2203, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1916, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2069, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2339, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1952, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2358, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2120, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2283, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2020, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1634, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1884, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2319, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1840, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1805, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1817, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2257, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2277, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2208, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2184, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1995, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1903, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2374, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2287, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2034, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2359, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2071, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1956, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2182, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2250, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2210, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2186, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2352, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1846, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2121, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2798, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2211, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2421, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2612, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1915, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2100, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2670, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2347, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1745, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2351, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1746, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1906, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2205, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2404, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2674, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2169, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2506, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2101, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1900, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2327, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2208, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2004, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2557, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2231, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2097, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2490, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2522, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2005, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1985, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2450, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1890, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1822, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2579, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2846, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2454, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2027, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2674, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2149, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2017, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1650, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2720, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2007, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2751, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2597, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2302, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2379, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2296, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2412, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2573, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2024, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1990, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2255, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2330, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2073, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1848, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2010, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2423, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2392, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1927, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1993, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1927, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2364, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2693, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2088, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2883, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2856, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1904, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2132, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2049, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2523, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2389, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1914, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2230, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2692, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1961, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2343, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2218, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2198, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1721, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2778, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2110, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1804, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1972, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2692, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2628, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2440, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1469, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2863, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2583, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2166, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2361, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2270, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2434, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2265, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2503, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2185, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1991, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1792, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1673, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1841, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2096, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2070, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2420, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2519, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1647, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2014, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2173, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2458, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2169, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1953, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2318, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2680, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2197, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2266, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2123, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1975, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2050, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1521, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2287, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1911, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2419, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1693, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2653, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2023, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2175, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2150, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2116, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1652, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1751, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2225, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2408, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2316, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2184, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2115, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1713, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2369, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1785, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2130, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1948, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2693, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2012, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2307, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2192, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1778, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2268, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2576, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2206, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2260, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2551, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2043, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2084, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2472, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1746, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2020, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2294, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2158, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2205, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2104, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2302, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2350, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1829, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2170, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1774, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2094, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2287, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1896, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2078, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2464, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2376, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2293, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1735, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2223, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2304, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2447, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2216, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2467, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1677, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2038, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2084, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2419, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2718, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1964, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2401, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2197, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2241, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2161, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2849, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2331, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1656, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1953, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2079, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2133, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2721, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2106, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2108, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2043, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1712, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2248, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2396, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2308, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2191, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2325, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2090, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2535, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1932, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2008, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1859, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1857, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2251, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2142, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2911, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2375, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2103, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2435, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2233, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2080, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2446, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2129, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2450, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2503, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1783, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1871, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1772, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1900, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2162, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2310, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1974, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1897, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1721, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1842, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2491, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1873, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2404, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1922, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2613, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1877, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2527, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2391, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2159, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2207, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2684, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2075, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2510, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2638, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2104, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1932, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2073, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1858, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2028, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2362, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2528, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1945, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2218, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2195, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2168, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2225, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2209, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1870, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2487, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1863, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2252, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1873, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1946, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1711, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2397, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2027, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2049, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2200, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2405, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2339, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2017, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2774, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2641, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2370, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2363, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1967, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2381, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2189, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2167, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2774, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2330, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2097, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2690, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2325, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2356, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1923, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2137, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2042, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1799, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2323, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2493, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1902, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2045, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2272, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2174, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2228, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2168, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2141, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2023, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2511, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1926, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2182, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2396, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2791, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2135, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1721, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2500, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2406, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1938, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2142, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1959, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2193, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2426, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1853, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2516, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2432, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2313, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2015, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2387, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2110, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2149, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2102, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2079, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2261, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2601, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2147, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2035, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2310, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2220, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2572, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2319, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2588, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2569, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2071, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2117, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2409, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1783, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2087, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1719, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2060, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1785, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2218, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2168, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2004, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1546, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2283, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2314, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1901, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2309, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2446, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2041, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2586, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2633, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1709, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2176, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1438, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2172, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2288, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2326, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1696, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2428, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1983, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2415, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1936, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2188, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1952, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2508, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2020, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2092, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2013, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2320, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2192, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1474, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2362, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1920, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2316, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1995, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2166, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2524, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2178, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2375, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1862, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2081, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2298, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2629, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2278, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2369, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2605, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2169, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2111, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1854, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2465, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2288, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2130, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2178, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2256, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2205, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2115, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2117, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2153, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1867, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2235, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2099, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1883, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1939, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2480, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2194, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2314, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2190, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2064, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2085, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1966, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2448, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1996, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2138, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2368, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2082, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2663, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2077, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2046, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2499, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2266, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2200, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1914, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2115, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1902, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1868, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2142, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2030, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1897, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2018, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1672, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2567, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2301, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2161, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2399, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2282, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1929, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2112, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1835, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2027, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2664, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2114, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2539, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2044, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2573, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2342, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2285, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2519, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1961, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2482, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2370, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1657, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2244, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2196, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2577, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2305, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2628, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2359, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2119, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2635, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2264, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2660, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2334, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2181, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1928, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1940, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2204, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1897, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1637, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2612, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2142, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2348, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1983, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2215, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2076, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2658, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1639, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2143, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1918, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2163, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2156, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2329, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2431, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2061, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2702, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2421, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2336, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2704, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2398, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2342, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2415, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2224, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1859, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1765, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2533, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2236, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1684, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1918, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2225, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1727, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2377, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1955, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2675, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1682, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2514, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2354, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2680, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2520, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2337, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2324, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1806, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1959, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2242, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1763, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2021, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2103, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2407, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2522, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2039, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2196, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2513, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2744, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2525, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2459, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2214, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2553, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2194, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2436, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2402, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1943, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2586, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2172, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2292, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1970, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2392, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2644, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2073, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2484, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1838, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2386, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2370, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1876, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2662, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1869, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2361, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2101, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2121, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1865, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1849, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2020, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1610, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1935, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1697, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2041, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2227, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2124, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2364, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1730, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2158, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2153, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2179, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2333, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2313, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2355, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2559, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2763, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2379, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1884, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2100, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1432, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2590, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2141, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1773, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1720, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1939, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2101, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2564, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2151, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1921, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2435, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1985, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2179, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1983, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2003, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1864, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2692, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2273, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2274, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1896, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1920, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1763, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2091, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2365, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2214, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1702, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2035, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2388, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2238, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2059, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2064, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2129, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1823, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1919, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2300, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1861, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1955, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2029, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1990, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1774, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2014, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2428, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2401, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2724, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2290, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2203, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2051, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2144, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2019, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2530, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1960, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2138, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2344, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2298, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2540, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2023, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2130, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2104, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2348, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1590, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2034, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2024, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2485, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2048, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2218, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2048, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2275, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1991, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2262, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2085, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2083, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2068, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2342, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2320, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2009, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2081, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2010, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2549, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2251, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2172, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2230, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2307, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2373, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2013, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2095, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2592, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2120, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1907, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2047, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2033, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1999, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2145, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1990, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1661, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2164, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2179, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2160, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1884, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1951, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1896, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2553, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2052, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2336, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1907, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2399, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2291, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2542, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1961, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1878, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1951, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2099, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2067, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2063, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2510, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2032, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2425, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2362, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2173, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2119, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2401, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2267, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2205, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2373, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2015, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2135, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1692, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2440, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2153, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2165, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1994, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2401, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2039, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1690, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1901, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2178, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1945, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1950, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2155, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1965, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1987, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2082, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1899, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2443, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2105, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2167, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1898, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1899, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2566, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2319, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2450, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2478, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2354, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2271, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2054, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2486, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2227, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2646, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2298, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1904, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1954, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2011, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2118, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2574, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2347, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2273, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2235, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1930, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2279, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2155, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2258, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1869, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1852, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2378, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2815, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2279, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1781, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2563, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1613, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2162, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2208, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2000, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2693, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2541, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.1767, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2121, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2226, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2048, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2011, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2025, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2156, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2107, grad_fn=<MseLossBackward0>)\n",
            "Training loss:  tensor(0.2012, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1\n",
        "test_dl = DataLoader(test_ds, batch_size, shuffle=True)\n",
        "next(iter(test_dl))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIFuRtL9hRHD",
        "outputId": "0c3aa55b-f975-451a-abdc-7af936ee8216"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[  5.0000, 756.0000,   0.0000,   2.0000,   0.0000,   0.0000,  -1.0000,\n",
              "            5.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
              "            1.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
              "            0.0000,   8.0000,  78.0000,   1.0000,   1.0000,  33.0000,   0.0000,\n",
              "            1.0000,   2.0000,   1.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
              "            0.0000,   0.0000,   0.0000,  20.0000,   0.0000,   0.0000,   0.0000,\n",
              "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,  10.0000,   0.0000,\n",
              "            2.0000,   2.0000,   0.0000,   0.0000,   0.0000,  34.0000,   0.0000,\n",
              "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
              "            0.0000,   0.0000,   0.0000,   0.0000, 770.1795,  39.0000,   3.0000,\n",
              "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
              "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
              "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
              "            0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
              "            1.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
              "            0.0000,   1.0000,   1.0000,   0.0000,   0.0000,   0.0000,  13.0000]]),\n",
              " tensor([0.])]"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "error_s = 0\n",
        "with torch.no_grad():\n",
        "  for x_tst, y_tst in test_dl:\n",
        "    #print(x_tst.shape)\n",
        "    prediction = model_l(x_tst)\n",
        "    #print(prediction.round(), \"True estimate:\", y_tst)\n",
        "    error_s += abs(prediction.round()-y_tst)\n",
        "\n",
        "error = error_s/len(test_ds)\n",
        "print(\"Accuracy:\", 1-error)"
      ],
      "metadata": {
        "id": "KePSiKlP6Gw0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d83114c-9691-4b21-b540-c3ec56a3fa51"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: tensor([[0.7406]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hE_IlynTWUGs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}